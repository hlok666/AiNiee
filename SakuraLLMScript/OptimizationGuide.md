# SakuraLLM 性能优化指南

## 0. 前言
SakuraLLM 默认设置比较保守，性能较差。

通过本文的优化，您可以获得多至5倍的性能提升。

本文假定您已经使用 SakuraLLM 官方提供的一键包进行部署，并且基本翻译流程已经调通。

如果您使用其他部署方式或需要基础部署教程，请自行调整或寻找相应文档。

## 1. 选择合适的模型
根据显存大小下载适合的模型并放入 SakuraLLM 文件夹。

7B 模型的翻译能力较差，除非显存受限，否则应避免使用。

32B 模型与 14B 模型相比，速度显著降低，但是翻译能力的提升有限，除 4090 用户以外不推荐使用。

| 显存大小         | 推荐模型大小 | 推荐模型及链接                                             |
|:---------------:|:-----------:|:---------------------------------------------------------:|
| 8G              | 7B          | [GalTransl-7B-v1-IQ4_XS.gguf](https://huggingface.co/SakuraLLM/GalTransl-v1/blob/main/GalTransl-7B-v1-IQ4_XS.gguf) |
| 11G/12G/16G/24G | 14B         | [Sakura-14B-Qwen2beta-v0.9.2-GGUF](https://huggingface.co/SakuraLLM/Sakura-14B-Qwen2beta-v0.9.2-GGUF/blob/main/sakura-14b-qwen2beta-v0.9.2-iq4xs.gguf) |

## 2. 启用 SakuraLLM 多实例支持
从 [SakuraLLMScript目录](/SakuraLLMScript/) 下载 `common.bat` 以及与您的显存大小对应的启动脚本，并将其放入 SakuraLLM 文件夹。

| 显存大小  | 启动脚本名称            |
|:--------:|:----------------------:|
| 8G       | 00_RUN_8G_7B           |
| 11G      | 00_RUN_11G_14B         |
| 12G      | 00_RUN_12G_14B         |
| 16G      | 00_RUN_16G_14B         |
| 24G      | 00_RUN_24G_14B         |

> [!IMPORTANT]
>
> 双击启动脚本，启动支持多实例的 SakuraLLM，请选择与脚本相对应大小的模型。
> 
> 每个人的系统环境不同，显存占用有波动是正常现象，如遇显存不足，建议在使用过程中关闭浏览器的硬件加速及其他占用显存的软件。

## 3. 设置 AiNiee 参数
确保 AiNiee 已更新至最新版本（>= 4.71.0），启动应用，并根据显存大小设置以下选项：

| 选项 | 设置 |
|------|:----:|
| 翻译设置 - 发送设置 - 使用 Tokens 模式      | 启用 |
| 翻译设置 - 发送设置 - 每次翻译 Tokens       | 360  |
| 翻译设置 - 发送设置 - 最大线程数            | 8    |
| 翻译设置 - 发送设置 - 错误重翻最大次数限制   | 1    |
| 翻译设置 - 发送设置 - 翻译流程最大轮次限制   | 10   |
| 翻译设置 - 专项设置 - 保留换行符            | 启用  |
| 翻译设置 - 专项设置 - 处理收尾非字符文本     | 启用  |

完成这些设置后，您已经完成了对性能的优化。

现在，4070 Ti Super 16G 级别的显卡上每小时可以处理 3M-5M 的 mtools 导出文本，基本实现即翻即玩。

## 4. 其他可选操作
* 从 [llama.cpp 的发布页](https://github.com/ggerganov/llama.cpp/releases) 下载最新版本并覆盖 `SakuraLLM\llama` 中同名文件；

## 5. 原理说明

默认情况下，SakuraLLM 以单实例模式运行。

受限于模型端上下文长度的限制，翻译任务单次发送长度仅有数百字符，这种程度的任务单线程是无法跑满具有较高性能的GPU的。

通过调整模型的启动参数，启用多实例支持，并一次性发送多个任务，可以一定程度上解决这个问题。

以下是 `common.bat` 中的启动参数示例：

```shell
.\llama\server.exe -m .\%model.name%.gguf -fa --no-mmap -cb -np %np% -c %ctx% -ngl %ngl% -a %model.name% --host 127.0.0.1
```

关键参数解释：

`-cb`
* 启动多实例支持；

`-np` 
* 实例数量，原则上应 <=8，在不爆显存且不爆上下文长度的前提下，值越大，性能越好；

`-c` 
* 上下文长度，应该设置为 实例数量 x 单任务上下文长度
* AiNiee 应用内的 “每次翻译 Tokens” 的值应设置为 单任务上下文长度 的一半或略低于一半；
* 理论上越长越好，但是受限于模型能力与显存限制，设置的过大没有实际意义，建议值为 960 <= 单任务上下文长度 <= 2048；
    
## 6. 如何寻找能发挥出你的设备的极限性能的参数搭配
待续，有空继续写...

~~其实也没啥好测的，你一个一个配置组合测过去，测完大概率发现还是一键脚本里的最好用~~
